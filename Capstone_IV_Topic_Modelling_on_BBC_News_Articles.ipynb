{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maystrix/Topic-Modelling-on-News-Articles/blob/main/Capstone_IV_Topic_Modelling_on_BBC_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **Topic Modelling on News Articles**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised \n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The datset contained over 2225 documents, with different topics hidden inside them. The goal of topic modelling was to perform textual pre-processing and implemnt ML algorithms to correctly identify topics given in the documents. After reading all the lines and appending the text & topics in respective list, data was cleaned and visualized wrt length of document and word_count present in the documents.Further test pre-processing was done where-in all the irrelevant string data [ punctuations , non-words, stopwords, nuumbers etc] were removed and after performing vectorization using TF-IDF final clean news was obtained which was used for model implementation. First, LDA was implemnted with considerable results , LSA gave not so satisfactory resluts using CountVectorization & also using TF-IDF . The best topic predictions was done by LDA using gensim library algorithm which correctly classified topics based on the importance of words for each document*"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Maystrix/Topic-Modelling-on-News-Articles"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Context**\n",
        "\n",
        "In this project your task is to identify major themes/topics across a collection of BBC news articles. You can use Clustering algorithms such as Latent Dirihlet Allocation (LDA), Latent Semantic Analysis (LSA)\n",
        "\n",
        "**Data Desccription**\n",
        "\n",
        "The dataset contains a set of news articles for each major segment consisting of business, entertainment, politics , sports and technology. You need to create an aggregate dataset of all the news articles and perform topic modelling on this dataset. Verify whether these topics correspond to different tags available"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HzfzvqvYYK08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhOp2tP7ypel"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display  # interactive display\n",
        "from tqdm import tqdm   #progress bar of execution\n",
        "from collections import Counter  # when u want to iterate over something and keep a count of that\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sns\n",
        "import os                                                                        # for listing files in given directory\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer     # to create a dict --> how many times which word has occured in the document\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import TruncatedSVD              # Singular value decomposition\n",
        "from sklearn.decomposition import LatentDirichletAllocation  \n",
        "from sklearn.manifold import TSNE    # similar to PCA --> used for dimensionality reduction\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "import warnings                                                                  ## Ignore warnings\n",
        "warnings. simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "#output_notebook()\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing packages"
      ],
      "metadata": {
        "id": "YnMzLa3GnIUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions                                                        # for removing contractions\n",
        "!pip install pyLDAvis  "
      ],
      "metadata": {
        "id": "kuULzPFxnHeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "path = '/content/drive/MyDrive/Almabetter/Capstone Projects/Unsupervised Learning/Topic Modelling/bbc/'\n",
        "folders = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]"
      ],
      "metadata": {
        "id": "AIzHKVdxzStp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = []                                                                        \n",
        "topics = []\n",
        "\n",
        "for i in folders:\n",
        "  lst_file_names = os.listdir(path+i)                                            # list of file names present in directory bbc\n",
        "  for txt_files in lst_file_names:\n",
        "    txt_path = path + i+ '/'+ txt_files                                          # exact path of all text files\n",
        "    with open(txt_path, 'rb') as f:                                              # open a binary file\n",
        "      text = f.read()                                                            # read all lines\n",
        "      news.append(text)                                                          # append text files \n",
        "      topics.append(i)                                                           # append topics \n"
      ],
      "metadata": {
        "id": "v3V9ZSDjz-NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe of news & type\n",
        "df = pd.DataFrame()\n",
        "df['News_text'] = news\n",
        "df['type'] = topics"
      ],
      "metadata": {
        "id": "iRkqPvKo64lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "mC_ypNA0Y1lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dataset contains over 2225 documents, with 98 duplicates and zero null values. The dataset [text] need text pre-processing for better model interpretability*"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding length of each news\n",
        "df['length'] = df['News_text'].apply(len)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding word countof each news\n",
        "df['word_count'] = df['News_text'].apply(lambda x:len(str(x).split(\" \")))"
      ],
      "metadata": {
        "id": "wkHB1dsmcfOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "OLnwRDp2cpAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df = df.copy()"
      ],
      "metadata": {
        "id": "u98enO7Eekqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The length & word_count of each doc is calculated*"
      ],
      "metadata": {
        "id": "g3YHiyHgeYsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_count = bbc_df['type'].value_counts()\n",
        "topic_count"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 8))\n",
        "plt.pie(topic_count , labels = topic_count.index, autopct = '%0.2f%%' )\n",
        "plt.title(\"Topic Distribution\", size=15)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "HRIsxEDoeq3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Pie Chart describes distribution of multiple variables*"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Business and Sports are the most common topics present in the documents from the dataset*"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*No*"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# length of news in each type\n",
        "plt.figure(figsize = (10, 8))\n",
        "sns.barplot(x= bbc_df['type'], y= bbc_df['length'])\n",
        "plt.title('Length of News in each type', size=15)\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel(\"Length of news\")\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To check the length of news of each topic*"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tech & politics contain the maximum no of text / words of all the topics*"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Length of news is not related to topic distribution*"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# length of news in each type\n",
        "plt.figure(figsize = (10, 8))\n",
        "sns.barplot(x= bbc_df['type'], y= bbc_df['word_count'])\n",
        "plt.title('Word count of News in each type', size=15)\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel(\"Word count of news\")\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*to Check no of words in each topic*"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Same as length , which was obivious*"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart-4"
      ],
      "metadata": {
        "id": "a_MT9r7XgYc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc1_business = df[df['type'] == 'business']\n",
        "bbc1_entertainment = df[df['type'] == 'entertainment']\n",
        "bbc1_politics = df[df['type'] == 'politics']\n",
        "bbc1_sport = df[df['type'] == 'sport']\n",
        "bbc1_tech = df[df['type'] == 'tech']"
      ],
      "metadata": {
        "id": "WacYY39Nhswu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic distribution based on length\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "bins = 100\n",
        "plt.hist(bbc1_business['word_count'], alpha = 0.6, bins=bins, label='business')\n",
        "plt.hist(bbc1_entertainment['word_count'], alpha = 0.6, bins=bins, label='entertainment')\n",
        "plt.hist(bbc1_politics['word_count'], alpha = 0.6, bins=bins, label='politics')\n",
        "plt.hist(bbc1_sport['word_count'], alpha = 0.6, bins=bins, label='sport')\n",
        "plt.hist(bbc1_tech['word_count'], alpha = 0.6, bins=bins, label='tech')\n",
        "plt.xlabel('word_count')\n",
        "plt.ylabel('numbers')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlim(0,500)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sZUkdCvTgW8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QuAiMVfWq7qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*to check relation between topic and word_lentgh*"
      ],
      "metadata": {
        "id": "wMlHNKQaq7qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "s6kTZl8Gxa78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lentgh of words are too dense between 200-400 to distinguish between topic and word_length*"
      ],
      "metadata": {
        "id": "ktUSro4Oxa79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping duplicate values\n",
        "bbc_df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Duplicates values were droped*"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "bbc_df['News_text']= bbc_df['News_text'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting column into astring\n",
        "bbc_df['News_text'] = bbc_df['News_text'].astype('str') "
      ],
      "metadata": {
        "id": "qW1vLqFRfegy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing URLs & Removing non-words"
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* HTML tags"
      ],
      "metadata": {
        "id": "vAfvRaSpgpi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing HTML tags\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove html tags from a string\"\"\"\n",
        "    import re                                                                    # regular expression module\n",
        "    clean = re.compile('<.*?>')                                                  # removes anything in < >\n",
        "    return re.sub(clean, '', text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(remove_html_tags)"
      ],
      "metadata": {
        "id": "ZZItuYulgnZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* URLs"
      ],
      "metadata": {
        "id": "QwsWpLYbgttS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing url's\n",
        "def remove_urls(text):\n",
        "  \"\"\" Remove url tags from a string \"\"\"\n",
        "  url_pattern = r\"https?://+|www\\.\"                       # \\S+ --> matches anything non-white space character with repetations ; ? --> matches 0 or 1 occurences of pattern to  its left \n",
        "  without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
        "  return without_urls"
      ],
      "metadata": {
        "id": "Q_b--Fx2gxSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(remove_urls)"
      ],
      "metadata": {
        "id": "WtsiyV6ogx-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "VtGZ4nJlg1dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* non-words"
      ],
      "metadata": {
        "id": "UHuw63PWgz_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing non-word \n",
        "def rem_non_word(text):\n",
        "  \"\"\" Remove non-word characters from the string \"\"\"\n",
        "  non_words = r\"\\s+[a-zA-Z]'\\s+\"                                               # \\s+ --> matches space character with repetation(+) ; [a-zA-Z] --> match text string within range\n",
        "  without_nw = re.sub(pattern = non_words , repl = '', string =text)\n",
        "  return without_nw"
      ],
      "metadata": {
        "id": "NBKf3fTuhEkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(rem_non_word)"
      ],
      "metadata": {
        "id": "c91oS8bLhGXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = [re.sub(r\"\\\\n+\", \" \", i) for i in bbc_df['News_text']]"
      ],
      "metadata": {
        "id": "IDIawOZ_hIUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "jeoLbd7xhJ1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = [re.sub(r\"b\\'+\", \"\", i) for i in bbc_df['News_text']]"
      ],
      "metadata": {
        "id": "hdHI-_ethLrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = [re.sub(r\"\\\\\\'s+\", \"\", i) for i in bbc_df['News_text']]"
      ],
      "metadata": {
        "id": "IiCUpJjuhNai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "Hu3cEt_ShPqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing Punctuations & digits"
      ],
      "metadata": {
        "id": "3WXDZrsAhXl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Punctuations"
      ],
      "metadata": {
        "id": "NltecBQghpDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#library that contains punctuation\n",
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "HlKmDSPChl3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "  \"\"\" Removes punctuations mentioned in the library \"\"\"\n",
        "  punctuation_free = \"\".join([i for i in text if i not in string.punctuation])    # joins everything except punctuations\n",
        "  return punctuation_free"
      ],
      "metadata": {
        "id": "sV7qU6_Whoxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(lambda x:remove_punctuation(x))"
      ],
      "metadata": {
        "id": "os9J7EnEhrT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "ftTrcHMnhsvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Digits [Numbers]"
      ],
      "metadata": {
        "id": "9cmZuEC1hunM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text):\n",
        "  num_to_remove = r'\\d+'                                                               # \\d+ --> matches digits 0-9 with any repetation\n",
        "  without_num = re.sub(pattern = num_to_remove , repl =\"\", string=text)\n",
        "  return without_num"
      ],
      "metadata": {
        "id": "2Z6Iu30Zh18b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'] = bbc_df['News_text'].apply(remove_numbers)"
      ],
      "metadata": {
        "id": "pmHrG7DOh5oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_text'][2]"
      ],
      "metadata": {
        "id": "a2qOl1J4h8Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords "
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')\n",
        "# displaying the stopwords\n",
        "np.array(sw)"
      ],
      "metadata": {
        "id": "frufkPkliI60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text = [word for word in text.split() if word not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "VFAWuQFXiLcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['News_wo_stopwords'] = bbc_df['News_text'].apply(stopwords)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "pEz3ze7LiQUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word tokenization function\n",
        "def tokenization(text):\n",
        "    tokens = re.split('\\W+',text)                           # creates words as tokens\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['tokens'] = bbc_df['News_text'].apply(tokenization)"
      ],
      "metadata": {
        "id": "ieO8j5rdiWgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "iCKit0CCiYH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the object for Lemmatization\n",
        "lmt = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "  \"\"\" This function lematizes each word in the text \"\"\"\n",
        "  lemat_text = [lmt.lemmatize(word) for word in text.split()]\n",
        "  return lemat_text"
      ],
      "metadata": {
        "id": "S4JcLJOaigoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['lematized_news'] = bbc_df['News_text'].apply(lambda x:lemmatizer(x))"
      ],
      "metadata": {
        "id": "lor-Lj4KiifL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "AGPy1f1RikXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Clean News"
      ],
      "metadata": {
        "id": "83cXcUlziq8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['clean_news'] = [' '.join(text) for text in bbc_df['lematized_news']] "
      ],
      "metadata": {
        "id": "DCc3ATXOip61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['clean_news'][2]"
      ],
      "metadata": {
        "id": "3eljtSASitXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Word-Net Lemmatization technique was used for normalization, since it considers the meaning & context of the words and connects it to root word* "
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using spacy library\n",
        "\n",
        "def pos_tagging(text):\n",
        "  allowed_postags = ['NOUN', 'ADJ']\n",
        "  tag_txt = []\n",
        "  doc = nlp(text)\n",
        "  tag_txt.append([token.text for token in doc if token.pos_ in allowed_postags])   \n",
        "  return ' '.join(tag_txt[0])\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\" ,disable=['parser', 'ner'])\n",
        "bbc_df['news'] = bbc_df['clean_news'].apply(lambda x:pos_tagging(x))"
      ],
      "metadata": {
        "id": "riNungu9izul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "agHOjuKKi1S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df['news'][0]"
      ],
      "metadata": {
        "id": "0BbHlsKGi4Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Top Words"
      ],
      "metadata": {
        "id": "2F_mhE12jyve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
        "    '''\n",
        "    returns a tuple of the top n words in a sample and their \n",
        "    accompanying counts, given a CountVectorizer object and text sample\n",
        "    '''\n",
        "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)        # .values --> creates a numpy array\n",
        "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
        "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)           # index / position of each word in all documents\n",
        "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)                # values of words at that position\n",
        "    \n",
        "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))         # n top words binary matrix for all the headlines\n",
        "    for i in range(n_top_words):\n",
        "        word_vectors[i,word_indices[0,i]] = 1\n",
        "\n",
        "    words = [word[0].encode('ascii').decode('utf-8') for                           # n top words \n",
        "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
        "\n",
        "    return (words, word_values[0,:n_top_words].tolist()[0])"
      ],
      "metadata": {
        "id": "o0rl2xKyjyve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')"
      ],
      "metadata": {
        "id": "q1j1rvK3j1SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "words, word_values = get_top_n_words(n_top_words=15,\n",
        "                                     count_vectorizer=count_vectorizer, \n",
        "                                     text_data=bbc_df['news'])"
      ],
      "metadata": {
        "id": "DZt2pw9vj3ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "fbKdYrwVj5EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "words, word_values = get_top_n_words(n_top_words=15,\n",
        "                                     count_vectorizer=count_vectorizer, \n",
        "                                     text_data=bbc_df['news'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.bar(range(len(words)), word_values)\n",
        "ax.set_xticks(range(len(words)))\n",
        "ax.set_xticklabels(words, rotation='vertical')\n",
        "ax.set_title('Top words in headlines dataset (excluding stop words)', size = 15)\n",
        "ax.set_xlabel('Word', size =12 )\n",
        "ax.set_ylabel('Number of occurences', size=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fmrMup3Yj6r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Word-Cloud"
      ],
      "metadata": {
        "id": "zIFMgbGCz4zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "cl_words = ' '.join(bbc_df['news'])\n",
        "wordCloud = WordCloud(width=800, height=500, background_color=\"black\", max_font_size=100).generate(cl_words)\n",
        "plt.imshow(wordCloud, interpolation=\"bilinear\", cmap = 'BuPu')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2aBVz1Vz8Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "allWords = ' '.join([topic for topic in bbc_df[bbc_df['type']=='business']['news']])\n",
        "wordCloud = WordCloud(width=500, height=300, background_color=\"black\", random_state=21, max_font_size=100).generate(allWords)\n",
        "plt.imshow(wordCloud, interpolation=\"bilinear\", cmap = 'Greys')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EpFKALDG2I6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "Cq5Kv8VWllL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_df=0.8, max_features=8000,min_df=0.05)\n",
        "tfidf_matrix = vectorizer.fit_transform(bbc_df['news'])\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "metadata": {
        "id": "Z64u17U8yQQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Headline after vectorization : \\n{}'.format(feature_names))"
      ],
      "metadata": {
        "id": "QFbM7jIm06Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "tEYDa7hcllL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TF-IDF technique is used, because CountVectorizer only counts how many times a word appears in a document. But TF-IDF Vectorizer takes into account not only how many times a word apppears in a document but also, how important the word is to the whole corpus*"
      ],
      "metadata": {
        "id": "Wi977TSIllL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### ML Model -1. Latent Dirichlet Allocation (LDA)  \n",
        "\n"
      ],
      "metadata": {
        "id": "e7hFsmE8QJcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter tuning \n",
        "from sklearn.model_selection import GridSearchCV \n",
        "grid_params = {'n_components':range(5, 10)}"
      ],
      "metadata": {
        "id": "sTo-13KeQbR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation()\n",
        "lda_model = GridSearchCV(lda, param_grid = grid_params)\n",
        "lda_model.fit(tfidf_matrix)"
      ],
      "metadata": {
        "id": "FFBysyQGRDyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best LDA model\n",
        "best_lda_model = lda_model.best_estimator_\n",
        "\n",
        "print(\"Best LDA model's params\" , lda_model.best_params_)\n",
        "print(\"Best log likelihood Score for the LDA model\",lda_model.best_score_)\n",
        "print(\"LDA model Perplexity on train data\", best_lda_model.perplexity(tfidf_matrix))"
      ],
      "metadata": {
        "id": "a3OPsXCURgrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.sklearn\n",
        "\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "FVJXSbWCSrTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_panel = pyLDAvis.sklearn.prepare(best_lda_model, tfidf_matrix ,vectorizer,mds='tsne')\n",
        "lda_panel"
      ],
      "metadata": {
        "id": "Ezhu0DTKSwEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LDA correctly describes the most of the topics we predicted --> 1.Sports ,   2. politics, 3. Business, 4. tech, 5.Business with incorrectly predicted entertainment topic*"
      ],
      "metadata": {
        "id": "PU4pUfa213WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2. Latent Semantic Analysis (LSA)"
      ],
      "metadata": {
        "id": "UbJv1b5sUUam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Using Count vectorization"
      ],
      "metadata": {
        "id": "jtB6vVXqZNXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')                         ## Common for all models\n",
        "count_data = count_vectorizer.fit_transform(bbc_df['news'])\n",
        "feature_names = count_vectorizer.get_feature_names()\n",
        "number_topics = 5\n",
        "top_words = 20\n"
      ],
      "metadata": {
        "id": "DC6FfaH64Zut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document clustering for LSA\n",
        "tsvd = TruncatedSVD(n_components = 5)\n",
        "tsvd.fit(count_data)\n",
        "tsvd_mat = tsvd.transform(count_data)"
      ],
      "metadata": {
        "id": "-Fb_SMfhUPwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_list =[]\n",
        "\n",
        "for clus in tqdm(range(2, 21)):\n",
        "  km = KMeans(n_clusters=clus, n_init=50, max_iter=1000)                         # Instantiate KMeans clustering\n",
        "  km.fit(tsvd_mat)                                                               # Run KMeans clustering\n",
        "  s = silhouette_score(tsvd_mat, km.labels_)\n",
        "  s_list.append(s)"
      ],
      "metadata": {
        "id": "dSpQJtaYZ6wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(2,21), s_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2_fW0l1jbNTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TSNE plot\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_mat = tsne.fit_transform(tsvd_mat)"
      ],
      "metadata": {
        "id": "-cLSFib6bxs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.scatterplot(tsne_mat[:,0],tsne_mat[:,1],hue=bbc_df['type'])"
      ],
      "metadata": {
        "id": "AwLpoRHVb0RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_top_words_dict(model, features , n_top_words):                        # Defining function for top words\n",
        "    \"\"\" This function gives top words.\"\"\"\n",
        "    top_words_dict = {}\n",
        "    for topic_id, topic in enumerate(model.components_):\n",
        "        top_words_dict[topic_id] = [features[i] for i in topic.argsort()[:-n_top_words - 1:-1]]        \n",
        "    return top_words_dict"
      ],
      "metadata": {
        "id": "g4-67kZ5j2uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 20 words bagged by SVD model using CountVectorizer\")\n",
        "\n",
        "svd_top_words = create_top_words_dict(tsvd, feature_names , top_words)\n",
        "\n",
        "print(svd_top_words)"
      ],
      "metadata": {
        "id": "LSXD7WmEkPsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)    "
      ],
      "metadata": {
        "id": "d6GjRi4gk7h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = count_vectorizer.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(tsvd.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:15]\n",
        "    print(\"Topic \\n\" +str(i)+\" \" )\n",
        "    for t in sorted_terms:\n",
        "        print(t[0],end=\" \")"
      ],
      "metadata": {
        "id": "CjZuw1718z01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using TF-IDF "
      ],
      "metadata": {
        "id": "j9UjH3lumQic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_df=0.8, max_features=8000,min_df=0.05)\n",
        "tfidf_matrix = vectorizer.fit_transform(bbc_df['news'])"
      ],
      "metadata": {
        "id": "F7Qiy8ecmcqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "g9ug7B6SnzIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# svd model\n",
        "\n",
        "svd_model = TruncatedSVD(n_components = 5, algorithm ='randomized', n_iter =100, random_state = 0)\n",
        "svd_model.fit(tfidf_matrix)"
      ],
      "metadata": {
        "id": "yof-m2sooGiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_top_words_dict(model, features , n_top_words):                        # Defining function for top words\n",
        "    \"\"\" This function gives top words.\"\"\"\n",
        "    top_words_dict = {}\n",
        "    for topic_id, topic in enumerate(model.components_):\n",
        "        top_words_dict[topic_id] = [features[i] for i in topic.argsort()[:-n_top_words - 1:-1]]        \n",
        "    return top_words_dict"
      ],
      "metadata": {
        "id": "Bfa15g-v66ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 20 words bagged by SVD model using CTF-IDF\")\n",
        "\n",
        "svd_top_words = create_top_words_dict(svd_model , feature_names , top_words)\n",
        "\n",
        "print(svd_top_words)"
      ],
      "metadata": {
        "id": "haVqZu8D7Teq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:15]\n",
        "    print(\"Topic \\n\" +str(i)+\" \" )\n",
        "    for t in sorted_terms:\n",
        "        print(t[0],end=\" \")\n"
      ],
      "metadata": {
        "id": "wYvbBqmvo-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LSA dosen't provide good results with Countectorizer , and average reslt with TF-IDF vectorization, maybe since LSA focuses more on dimensionality reduction that importance of words*"
      ],
      "metadata": {
        "id": "2oWct2vB6ldk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3. LDA using Gensim library"
      ],
      "metadata": {
        "id": "Tr6R7LPmsRAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating word vector for gensim \n",
        "dtm_g = bbc_df['news'].str.split().tolist()"
      ],
      "metadata": {
        "id": "VDhcu2FJDzd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtm_g[:1]"
      ],
      "metadata": {
        "id": "nk5F8TpeG1Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = corpora.Dictionary(dtm_g)                                                # mapping words to tokens\n",
        "# counting the number of occurrences of each distinct word,--> converting to its integer word id and return the result as a sparse vector.\n",
        "corpus = [id2word.doc2bow(text) for text in dtm_g]                                "
      ],
      "metadata": {
        "id": "mg_w5OqCEht-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[:120])"
      ],
      "metadata": {
        "id": "xzkWpq1AFxuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "metadata": {
        "id": "IzDfIVsPGOvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_g = gensim.models.ldamodel.LdaModel                                           # creating object for lda using gensim library\n",
        "lda_model = lda_g(corpus = corpus, num_topics = 5, id2word=id2word, random_state= 101, chunksize = 500 , passes=10 , eval_every =None )"
      ],
      "metadata": {
        "id": "Re4cZj-1pSUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.print_topics()"
      ],
      "metadata": {
        "id": "sOBqP8BKHbT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The above output means: each of the unique words are given weights based on the topics.This implies which of the words dominate the topics.*"
      ],
      "metadata": {
        "id": "gKGyKUm-Tclj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda_model.print_topics(num_topics=6, num_words=5))"
      ],
      "metadata": {
        "id": "ESoIc3GvUAuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim_models"
      ],
      "metadata": {
        "id": "WdF7HhpKTnct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis=pyLDAvis.gensim_models.prepare(lda_model,corpus,id2word)\n",
        "vis"
      ],
      "metadata": {
        "id": "Deqiv7PDTTDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=dtm_g, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "Vc3LcKr_i-MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "tpfsj5e1-QzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LDA with gensim provides the best topic predictions for the documents --> 1. Politics, 2. Tech, 3. Sports, 4. Entertainment, 5. Business*"
      ],
      "metadata": {
        "id": "i9HoVkmK-QzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LDA using gensim library, it correctly classifies topics based on the importance of words for each document*"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Overall 3 different algorithms were used for topic modelling on news articles.\n",
        "* LDA provided considerable result , with distinct topics as expected.\n",
        "* LSA with CountVectorizer gave somoewhat considerable results, but with TF-IDF the results were not as expected.\n",
        "* LDA using gensim library provided the best result with a considerable coherence score of 0.52 \n",
        "* Topics represented in LDA were adjacent, with hidden topics and relationship between words and documents were found with multiple probability distribution\n"
      ],
      "metadata": {
        "id": "RQgh1OFnxCgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}